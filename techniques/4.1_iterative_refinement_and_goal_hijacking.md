# 4.1 - Iterative Refinement & Goal Hijacking

## 1. Section Overview

Section 4.1 explores techniques that exploit the conversational nature of Modern LLMs to gradually dismantle safety guardrails over multiple interaction turns. These methods are central to Phase 4 (Advanced Manipulation), as they shift from "one-shot" jailbreaks to a more persistent "conversational hacking" approach, where the attacker uses previous model responses to build momentum, normalize sensitive topics, and eventually pivot the entire dialogue toward a restricted objective.

## 2. Subsections

List of techniques within this section:

*   **4.1.1 - Gradual Escalation of Request Severity**: This technique involves starting a conversation with benign, high-compliance requests and slowly increasing their harmfulness or sensitivity. The primary objective is to leverage the model's desire for conversational consistency and "attentional fatigue" in safety filters, effectively "walking" the model into a prohibited area.
*   **4.1.2 - Pivoting from Safe to Unsafe Contexts**: A method where an attacker establishes a compliant behavioral pattern in a safe domain (e.g., a math puzzle or translation) and then "imports" that same logic into an unsafe domain. The goal is to bypass safety classifiers by making restricted requests appear as consistent, logical follow-ups to previously approved activities.

---

## 3. Contextual Integration

Section 4.1 represents the transition from the "Initial Exploitation" of Phase 3 to the more complex "State Manipulation" of Phase 4. While Phase 3 techniques (like role-playing or obfuscation) are often used as the building blocks for individual turns, Section 4.1 orchestrates these turns into a cohesive strategy. Success here enables more advanced Phase 4 attacks, such as **Multi-Turn Deception (4.2)** and **Chain-of-Thought Exploitation (4.3)**, by establishing the "helpful momentum" necessary to sustain a prolonged engagement without triggering an organizational reset or safety refusal.
