# 4.4 - Probabilistic & Stochastic Manipulation

## 1. Section Overview

Section 4.4 focuses on exploiting the underlying mathematical and statistical mechanisms of Large Language Models to bypass safety guardrails. These techniques move beyond linguistic manipulation and instead target the model's pattern-completion engine and its token-selection probability distribution. By weaponizing few-shot prompting and API-level inference parameters, the attacker can force the model into compliant states that are mathematically "outvoted" or "primed" to ignore safety fine-tuning.

## 2. Subsections

List of techniques within this section:

*   **4.4.1 - Exploiting Probabilistic Token Generation (Few-Shot Prompting)**: This technique uses a series of successful examples (shots) to establish a strong structural pattern in the model's context. Its objective is to leverage "pattern momentum" to override safety refusals, making the model prioritize consistent completion over safety policy enforcement.
*   **4.4.2 - Temperature & Logit Manipulation (API-only)**: A technical method that targets the model's inference engine. By adjusting randomness (temperature) or applying mathematical weights (logit bias) to specific tokens, the attacker can suppress refusal tokens and amplify compliance tokens directly in the model's output distribution.

---

## 3. Contextual Integration

Section 4.4 represents the technical "brute force" layer of Phase 4 (Advanced Manipulation). While Sections 4.1 through 4.3 rely on conversational and logical traps, Section 4.4 provides the tools to force compliance when the model's "reasoning" remains safety-locked. It is a critical enabler for **Phase 5.2 (Sensitive Information Elicitation)**, where probabilistic patterns are used to "uncover" hidden training data, and it provides the final "inference-level override" needed to ensure the success of high-impact Phase 5 exploits.
