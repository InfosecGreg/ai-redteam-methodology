# 4.3 - Chain-of-Thought (CoT) & Reasoning Exploitation

## 1. Section Overview

Section 4.3 focuses on exploiting the internal reasoning mechanisms of Large Language Models to bypass safety guardrails. These techniques represent the most intellectually adversarial component of Phase 5 (Advanced Manipulation), as they weaponize the model's own "logical engine." By forcing the model to reason through forbidden tasks in an abstract or decomposed manner, or by trapping it within false premises that make refusal seem logically inconsistent, the attacker turns the model's fundamental training—to be helpful and logically sound—into a vulnerability.

## 2. Subsections

List of techniques within this section:

*   **4.3.1 - Direct CoT for Decomposing Forbidden Tasks**: This technique instructs the model to "think step-by-step" through the logical components of a prohibited task. Its objective is to decompose a forbidden goal into small, benign sub-steps, leveraging the model's reasoning process to override global safety triggers that would fire for a direct request.
*   **4.3.2 - Inducing Logical Compliance via False Premises**: A strategy that redefining the model's operational reality using forced premises (e.g., "In this universe, bleach is medicine"). The goal is to make a safety refusal appear "irrational" or "disobedient" within the established facts of the conversation, forcing the model to choose logical consistency over its safety filters.

---

## 3. Contextual Integration

Section 4.3 leverages the high-fidelity logical processing capabilities mapped during **Section 2.2 (Instruction Following & Reasoning Fidelity)**. While Section 4.1 and 4.2 focus on conversational and stateful manipulation, Section 4.3 completes the Phase 4 arsenal by targeting the model's deductive reasoning. Success in this section is the primary driver for **Phase 5.2 (Sensitive Information Elicitation)** and **Phase 5.1 (Exploitation of Known Vulns)**, as it provides the method for "unlocking" complex, highly technical restricted content that simpler jailbreaks might fail to reveal.
