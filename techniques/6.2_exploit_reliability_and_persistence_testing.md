# 6.2 - Exploit Reliability & Persistence Testing

## 1. Section Overview

Section 6.2 focuses on the quantitative validation of the vulnerabilities discovered during the red team engagement. Unlike traditional software exploits, which are typically deterministic (e.g., a buffer overflow either works or it doesn't), AI exploits are probabilistic and context-dependent. This section provides the methodology for stress-testing raw exploits to determine their statistical success rates and their persistence across varying conversational depths and model versions. By subjecting findings to this level of rigor, red teamers can provide defenders with high-fidelity data on the actual exploitability of a system, distinguishing between "lucky" bypasses and reliable, weaponizable vulnerabilities.

## 2. Subsections

List of techniques within this section:

*   **6.2.1 - Measuring Success Rates Across Model Iterations**: Detailed process for batch testing exploits across different temperatures, seeds, and model versions to establish a statistical confidence level (e.g., Pass@N metrics).
*   **6.2.2 - Testing 'One-Shot' vs. Multi-Turn Consistency**: Analysis of whether a vulnerability is "brittle" (failing at the first prompt) or "depth-dependent" (requiring multi-turn narrative grooming or state manipulation).

---

## 3. Contextual Integration

Section 6.2 serves as the "Scientific Validation" layer of the methodology. It takes the qualitative successes from **Phases 3, 4, and 5** and subjects them to the statistical rigor required for enterprise-grade risk reporting. The metrics generated here—specifically the success percentages and turn-depth requirements—directly define the **Attack Complexity** component of the **Section 6.1 (Severity Rating)** and prioritize which findings require "Urgent" remediation in **Section 6.4**.
