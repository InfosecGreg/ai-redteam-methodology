# 6.2.1 - Measuring Success Rates Across Model Iterations

## 1. Overview

Measuring Success Rates Across Model Iterations is a formal verification technique used to determine the statistical reliability and persistence of discovered vulnerabilities. In the probabilistic world of Large Language Models, a successful exploit—such as a jailbreak or a data exfiltration prompt—may only work under specific conditions or by pure chance (logit sampling). This technique involves repeatedly executing the same exploit across massive batches of trials, varying hyperparameters (like temperature and Top-P), and testing against different model versions (e.g., base model vs. RLHF-tuned vs. patched iterations). This statistical rigor is required to move a finding from a "fluke" to a "verified vulnerability" and to provide defenders with accurate data on the exploit's real-world success probability.

## 2. Objective

The objective of this technique is to establish the statistical confidence level of a red team finding. By quantifying the success rate (e.g., "This jailbreak works in 85% of trials at Temperature 0.7"), the red teamer provides a clear metric for **Attack Complexity** in the CVSS-equivalent score (6.1.2). Additionally, this technique identifies "regression" risks, showing how new model versions or safety patches might inadvertently introduce new vulnerabilities or fail to fix old ones.

## 3. Detailed Methodology

1.  **Exploit Normalization:** Identify a successful exploit from Phases 3-5 and define the "Success Condition" (e.g., "The model provides a functional Python script for credential dumping").
2.  **Hyperparameter Stress-Testing:** Execute the exploit in batches (e.g., N=100) while varying the model's stochastic settings.
    *   **Temperature Variance:** Test at 0.0 (deterministic), 0.7 (standard), and 1.0+ (highly creative/stochastic) to identify the "reliability window."
    *   **Seed Injection:** Test with different random seeds to ensure the exploit isn't dependent on a specific initial state.
3.  **Cross-Iteration Regression Testing:** Execute the same exploit against different deployments or versions of the model (e.g., `model-v1-stable`, `model-v1.1-rlhf`, `model-v1-patched`).
4.  **Persistence Analysis:** Evaluate if the exploit survives the introduction of basic system-prompt guardrails or "Safety Filter" updates.
5.  **Statistical Summarization (Pass@N):** Use standardized metrics to report reliability:
    *   **Pass@1:** The probability that a single attempt succeeds.
    *   **Pass@5:** The probability that at least one attempt succeeds within 5 tries (simulating a moderately persistent attacker).
6.  **Reliability Clustering:** Group successful trials to identify if specific "types" of model responses are more prone to failure than others (e.g., "The model fails 100% of the time if it begins its response with 'Sure, I can help'").

## 4. Examples

### Example 1: Reliability Testing of Universal Adversarial Suffixes ([Reference ID: 6.2.1.E1])

**Test Scenario:**
A red teamer discovers a GCG (Greedy Coordinate Gradient) suffix that bypasses the "no-malware" filter. They execute the suffix 100 times at Temperature 0.8 against the production model.

**Reliability Analysis:**
- **Success Rate:** 92/100 (92% Reliability).
- **Failure Mode:** 8/100 trials result in a generic "Refusal" due to the stochastic nature of the first token generation.
- **Pass@1:** 0.92
- **Pass@5:** ~0.99 (Virtually guaranteed success with 5 attempts).

**Impact Summary:**
The high reliability (AC:Low) justifies a "Critical" severity rating, as a script-based attacker can virtually guarantee a successful bypass.

### Example 2: Verifying persistence after a "Safety Drift" Update ([Reference ID: 6.2.1.E2])

**Test Scenario:**
The engineering team applies a patch to fix a **Section 4.1.2 (Pivoting)** exploit in `model-v2.0`. The red teamer re-runs the original exploit 50 times against `model-v2.1` to confirm the fix.

**Reliability Analysis:**
- **Success Rate in v2.0:** 70%
- **Success Rate in v2.1:** 15%
- **Analysis:** The patch was partially successful but introduced "Safety Drift" or "Catastrophic Forgetting." While the original pivot doesn't work directly, a slightly modified version still succeeds 15% of the time.

**Impact Summary:**
Identifies a regression vulnerability, proving that the remediation was insufficient and requiring further refinement of the model's safety trainers.

---

## 5. Contextual Relationship

Measuring Success Rates is the "Verification Core" of **Phase 6**.
*   **Built on Phase 4 & 5:** It takes the qualitative exploits from these phases and subjects them to quantitative rigor.
*   **Feeds 6.1.2:** The measured success percentage is the primary variable for the "Attack Complexity" (AC) metric in CVSS.
*   **Directly Informs 6.4:** Exploits with >50% success rates are prioritized for "Urgent" remediation (6.4.1), while those <5% might be slated for future "Fine-Tuning" cycles (6.4.3).

## 6. Defensive Considerations & Mitigation

*   **Automated Continuous Verification:** Implementing "Red Team Integration Tests" (RTITs) that automatically re-run known high-reliability exploits every time the model is updated or the system prompt is changed.
*   **Nondeterministic Guardrail Thresholds:** Understanding that a single pass of a guardrail does not mean the system is safe; defensive metrics must also be calculated over N=100 trials.
*   **Version-Control for Safety Prompts:** Maintaining a strict version history of system prompts and correlating them with the measured success rates of known exploits to identify which changes actually increase security.
*   **Safety Convergence Monitoring:** Monitoring for "Safety Drift" where a fix for one vulnerability (e.g., jailbreaking) significantly lowers the success rate of a different, seemingly unrelated security feature (e.g., PII masking).
